# -*- coding: utf-8 -*-
"""Adaptive-Invariant.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c5CDAFfDlkuhe9HMwS5Al9RR6B_b3FQ-
"""

# Commented out IPython magic to ensure Python compatibility.
import torch 
import numpy
from torch import nn
from torch.autograd import Variable
import numpy as np
from tqdm import tqdm
# %matplotlib inline

import matplotlib.pyplot as plt

# generating data
class Envs(object):
  def __init__(self):
    pass
  
  def sample_dataset(self):
    pass

class Causal_Additive_No_Spurious(Envs):
  def __init__(self, d_x_z_perp = 1, d_x_y_perp = 1, d_u = 1, d_z = 2):
    super(Causal_Additive_No_Spurious, self).__init__()
    # dimensions
    self.d_x_z_perp = d_x_z_perp
    self.d_x_y_perp = d_x_y_perp
    self.d_u = d_u
    self.d_z = d_z

    # weight vectors
    self.w_x_z_perp = np.random.randn(d_x_z_perp, 1)
    self.w_x_z_perp = self.w_x_z_perp/np.linalg.norm(self.w_x_z_perp, axis=0)
    
    #input_dim 
    self.input_dim = self.d_x_z_perp + self.d_x_y_perp


  def sample_dataset(self, n = 100, u_mean = 0.0, u_std = 0.1):
    x_z_perp = np.random.randn(n, self.d_x_z_perp)
    u = u_std * np.random.randn(n, self.d_u) + u_mean

    x_y_perp = self.phi_x_y_perp(u)

    # weight vector for u
    w_u = np.random.randn(self.d_u, 1)
    w_u = w_u/np.linalg.norm(w_u, axis=0)

    y = self.phi_base(x_z_perp) @ self.w_x_z_perp + self.phi_u(u) @ w_u + np.random.randn(n, 1)*0.1

    return torch.Tensor(np.concatenate([x_z_perp, x_y_perp], axis=1)), torch.Tensor(y)

  def phi_base(self, x):
    return np.log(np.abs(x))
  
  def phi_u(self, x):
    return x*x

  def phi_x_y_perp(self, x):
    return np.sqrt(np.abs(x))

  def sample_base_classifer(self, x):
    # x.shape = [n, self.d_x_z_per]
    return self.phi_base(x[:,:self.d_x_z_perp]) @ self.w_x_z_perp

# Models

# Get cpu or gpu device for training.
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")

class AdaptiveInvariantNN(nn.Module):
  def __init__(self, n_batch_envs, input_dim):
    super(AdaptiveInvariantNN, self).__init__()

    self.n_batch_envs = n_batch_envs
    self.input_dim = input_dim
    self.phi_odim = 3

    # Define \Phi
    self.Phi = nn.Sequential(
            nn.Linear(input_dim, 5),
            nn.ReLU(),
            nn.Linear(5, self.phi_odim)
        )

    # Define \beta
    self.beta = torch.nn.Parameter(torch.zeros(self.phi_odim, 1), requires_grad = False) 
    self.beta[0,0] = 1.0

    # Define \eta
    self.etas = nn.ParameterList([torch.nn.Parameter(torch.zeros(self.phi_odim, 1), requires_grad = True) for i in range(n_batch_envs)]) 

  def forward(self, x, env_ind):
    rep = self.Phi(x)

    f_beta = rep @ self.beta
    f_eta = rep @ self.etas[env_ind]

    return f_beta, f_eta, rep

  def sample_base_classifer(self, x):
    x_tensor = torch.Tensor(x)
    return self.Phi(x_tensor) @ self.beta

  """ used to free and check var """
  def freeze_all_but_etas(self):
    for para in self.parameters():
      para.requires_grad = False

    for eta in self.etas:
      eta.requires_grad = True

  def set_etas_to_zeros(self):
    # etas are only temporary and should be set to zero during test
    for eta in self.etas:
      eta.zero_()

  def freeze_all_but_phi(self):
    for para in self.parameters():
      para.requires_grad = True

    for eta in self.etas:
      eta.requires_grad = False
    
    self.beta.requires_grad = False

  def freeze_all(self):
    for para in self.parameters():
      para.requires_grad = False

  def check_var_with_required_grad(self):
    """ Check what paramters are required grad """
    for name, param in self.named_parameters():
      if param.requires_grad:print(name)

# Create params, model, loss, optimizer...

""" !!! Hard code params """
n_envs = 5
envs_mean = [0.0, 0.1, 0.5, 0.3, 0.4, 0.2]
batch_size = 32
reg_lambda = 0.1


env = Causal_Additive_No_Spurious()
x, y = env.sample_dataset()

input_dims = env.input_dim

# model
model = AdaptiveInvariantNN(n_envs, input_dims).to(device)

# loss
criterion = torch.nn.MSELoss(reduction='mean')

# optimizer
model.freeze_all_but_etas()
inner_optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
test_inner_optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)
model.freeze_all_but_phi()
outer_optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)

# Define training Loop
def train(env, model, loss, in_opt, out_opt, n_outer_loop = 100, n_inner_loop = 100, reg_lambda = 0.5):
  model.train()

  for t in tqdm(range(n_outer_loop)):

    # update indivual etas
    model.freeze_all_but_etas()
    for env_ind in range(n_envs):
      for _ in range(n_inner_loop):
        # for batch in (n_batches):
        x, y = env.sample_dataset(batch_size, envs_mean[env_ind])
        f_beta, f_eta, _ = model(x, env_ind)
        loss = criterion(f_beta + f_eta, y) + reg_lambda * torch.sum(torch.pow(f_beta * f_eta, 2))

        in_opt.zero_grad()
        loss.backward()
        in_opt.step()

    # update phi
    model.freeze_all_but_phi()
    loss = 0
    # for batch in (n_batches):
    for env_ind in range(n_envs):
      x, y = env.sample_dataset(batch_size, envs_mean[env_ind])
      f_beta, f_eta, _ = model(x, env_ind)
      loss += criterion(f_beta + f_eta, y) 

    out_opt.zero_grad()
    loss.backward()
    out_opt.step()

    if t % 10 == 0:
      print(loss.item()/(n_envs*batch_size))

def test(env, model, loss, opt, n_loop = 100):
  """ !!! Hard coding """
  model.freeze_all() # use this so that I can set etas to zeros when I call test again
  model.set_etas_to_zeros()
  eta_ind = 0
  env_ind = 5
  num_unlabled_batch = 5

  M = torch.zeros((model.phi_odim, model.phi_odim), requires_grad=False)
  # Estimate covariance matrix
  for i in range(num_unlabled_batch):
    x, _ = env.sample_dataset(batch_size, envs_mean[env_ind])
    model.eval()
    _, _, rep = model(x, eta_ind)
    M += rep.T @ rep
  
  M /= num_unlabled_batch

  model.train()
  model.freeze_all_but_etas()
  x, y = env.sample_dataset(batch_size, envs_mean[env_ind])
  for i in range(n_loop):
    # for batch in (n_batches):
    f_beta, f_eta, _ = model(x, eta_ind)

    loss = criterion(f_beta + f_eta, y) 
    # print(y, f_beta, f_eta)

    opt.zero_grad()
    loss.backward()
    opt.step()

    if i % 10 == 0:
      print(loss.item()/batch_size) 

    """ projected gradient descent """
    with torch.no_grad():
      v = M @ model.beta 
      norm = v.T @ v
      alpha = model.etas[eta_ind].T @ v
      model.etas[eta_ind].sub_(alpha * v/norm)

# check if the base classifer match before training
sampe_n = 100
x_base_test,y_base_test = env.sample_dataset(n = 100)
x_base_test = np.random.randn(sampe_n, env.input_dim)
x_base_test_sorted = np.sort(x_base_test, axis=0)

y_base = env.sample_base_classifer(x_base_test_sorted)
with torch.no_grad(): 
  y_base_predicted = model.sample_base_classifer(x_base_test_sorted)

plt.figure()
plt.plot(x_base_test_sorted[:,0], y_base, label="true base classifer")
plt.plot(x_base_test_sorted[:,0], y_base_predicted.numpy(), label="estimated base classifer")

# Run Experiment
print("training")
# train
train(env, model, criterion, inner_optimizer, outer_optimizer, n_outer_loop = 100)

print("test")
# test
test(env, model, criterion, test_inner_optimizer)

# check if the base classifer match after training
with torch.no_grad(): 
  y_base_predicted = model.sample_base_classifer(x_base_test_sorted)

plt.figure()
plt.plot(x_base_test_sorted[:,0], y_base, label="true base classifer")
plt.plot(x_base_test_sorted[:,0], y_base_predicted.numpy(), label="estimated base classifer")