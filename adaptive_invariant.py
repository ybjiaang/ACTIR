# -*- coding: utf-8 -*-
"""Adaptive-Invariant.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c5CDAFfDlkuhe9HMwS5Al9RR6B_b3FQ-
"""

# Commented out IPython magic to ensure Python compatibility.
import torch 
from torch import nn
from torch.autograd import Variable
import numpy as np
from tqdm import tqdm
import random
import argparse
# %matplotlib inline

import matplotlib.pyplot as plt

from syn_env import CausalAdditiveNoSpurious
from models.adap_invar import AdaptiveInvariantNN, AdaptiveInvariantNNTrainer


if __name__ == '__main__':
  torch.manual_seed(0)
  random.seed(0)
  np.random.seed(0)

  parser = argparse.ArgumentParser()

  parser.add_argument('--n_envs', type=int, default= 5, help='number of enviroments per training epoch')
  parser.add_argument('--batch_size', type=int, default= 32, help='batch size')
  parser.add_argument('--reg_lambda', type=float, default= 0.5, help='regularization coeff for adaptive invariant learning')

  parser.add_argument('--dataset', type=str, default= "syn", help='type of experiment')

  # synthetic dataset specifics
  parser.add_argument('--syn_dataset_train_size', type=int, default= 256, help='size of synthetic dataset per env')
  # parser.add_argument('--syn_dataset_test_size', type=int, default= 5, help='size of synthetic dataset per env')

  # misc
  parser.add_argument('-print_base_graph', type=bool, default=False, help='whether to print base classifer comparision graph, can only be used in 1 dimension')

  args = parser.parse_args()

  # Get cpu or gpu device for training.
  args.device = "cuda" if torch.cuda.is_available() else "cpu"
  print(f"Using {args.device} device")

  # create datasets
  if args.dataset == "syn":
    env = CausalAdditiveNoSpurious()
    args.n_envs = env.num_train_evns

    # create training data
    train_dataset = []
    for i in range(env.num_train_evns):
      x, y = env.sample_envs(i, n = args.syn_dataset_train_size)
      train_dataset.append((x,y))

    # create val dataset
    x, y = env.sample_envs(env.num_train_evns, n = args.syn_dataset_train_size)
    val_dataset = (x, y)

    # create test dataset
    x, y = env.sample_envs(env.num_train_evns + 1, n = 10)
    test_finetune_dataset = (x, y)

    x, _ = env.sample_envs(env.num_train_evns + 1, n = args.syn_dataset_train_size)
    test_unlabelled_dataset = (x,)

    x, y = env.sample_envs(env.num_train_evns + 1, n = args.syn_dataset_train_size)
    test_dataset = (x, y)

  # model
  input_dims = env.input_dim
  model = AdaptiveInvariantNN(args.n_envs, input_dims).to(args.device)

  # loss fn
  criterion = torch.nn.MSELoss(reduction='mean')

  trainer = AdaptiveInvariantNNTrainer(model, criterion, args.reg_lambda)
  
  # check if the base classifer match before training
  sampe_n = 100
  # x_base_test,y_base_test = env.sample_random_dataset(n = sampe_n)
  x_base_test,y_base_test = env.sample_envs(env.num_train_evns + 1, n = sampe_n)
  x_base_test_sorted = np.sort(x_base_test, axis=0)

  y_base = env.sample_base_classifer(x_base_test_sorted)
  with torch.no_grad(): 
    y_base_predicted = trainer.model.sample_base_classifer(x_base_test_sorted)

  if args.print_base_graph:
    plt.figure()
    plt.plot(x_base_test_sorted[:,0], y_base, label="true base classifer")
    plt.plot(x_base_test_sorted[:,0], y_base_predicted.numpy(), label="estimated base classifer")
    plt.savefig("comparision_before.png")

  # Run Experiment
  print("training...")
  # train
  trainer.train(train_dataset, args.batch_size)

  print("test...")
  # test

  trainer.model.set_etas_to_zeros()
  trainer.test(test_dataset)

  proj_gd_loss = 0.0
  gd_loss = 0.0
  for i in range(8):
    x, y = test_finetune_dataset

    partical_test_finetune_dataset = (x[i:i+1,:], y[i:i+1])

    print("prjected gradient descent")
    trainer.finetune_test(partical_test_finetune_dataset, test_unlabelled_dataset)
    proj_gd_loss+=trainer.test(test_dataset)

    print("regular gradient descent")
    trainer.finetune_test(partical_test_finetune_dataset, test_unlabelled_dataset, projected_gd=False)
    gd_loss+=trainer.test(test_dataset)

  print(proj_gd_loss/8, gd_loss/8)

  # check if the base classifer match after training
  with torch.no_grad(): 
    y_base_predicted = trainer.model.sample_base_classifer(x_base_test_sorted)

  if args.print_base_graph:
    plt.figure()
    plt.plot(x_base_test_sorted[:,0], y_base, label="true base classifer")
    plt.plot(x_base_test_sorted[:,0], y_base_predicted.numpy(), label="estimated base classifer")
    plt.savefig("comparision_after.png")
