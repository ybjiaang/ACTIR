# -*- coding: utf-8 -*-
"""Adaptive-Invariant.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c5CDAFfDlkuhe9HMwS5Al9RR6B_b3FQ-
"""

# Commented out IPython magic to ensure Python compatibility.
import torch 
from torch import nn
from torch.autograd import Variable
import numpy as np
from tqdm import tqdm
import argparse
# %matplotlib inline

import matplotlib.pyplot as plt

from syn_env import CausalAdditiveNoSpurious
from models.adap_invar import AdaptiveInvariantNN, AdaptiveInvariantNNTrainer


if __name__ == '__main__':
  parser = argparse.ArgumentParser()

  parser.add_argument('--n_envs', type=int, default= 5, help='number of enviroments per training epoch')
  parser.add_argument('--batch_size', type=int, default= 32, help='batch size')
  parser.add_argument('--reg_lambda', type=float, default= 0.5, help='regularization coeff for adaptive invariant learning')

  parser.add_argument('--dataset', type=str, default= "syn", help='type of experiment')

  # synthetic dataset specifics
  parser.add_argument('--syn_dataset_train_size', type=int, default= 128, help='size of synthetic dataset per env')
  parser.add_argument('--syn_dataset_test_size', type=int, default= 5, help='size of synthetic dataset per env')

  args = parser.parse_args()

  # Get cpu or gpu device for training.
  args.device = "cuda" if torch.cuda.is_available() else "cpu"
  print(f"Using {args.device} device")

  # create datasets
  if args.dataset == "syn":
    env = CausalAdditiveNoSpurious()
    args.n_envs = env.num_train_evns

    # create training data
    train_dataset = []
    for i in range(env.num_train_evns):
      x, y = env.sample_envs(i, n = args.syn_dataset_train_size)
      train_dataset.append((x,y))

    # create val dataset
    x, y = env.sample_envs(env.num_train_evns, n = args.syn_dataset_train_size)
    val_dataset = (x, y)

    # create test dataset
    x, y = env.sample_envs(env.num_train_evns + 1, n = args.syn_dataset_test_size)
    test_dataset = (x, y)

  # model
  input_dims = env.input_dim
  model = AdaptiveInvariantNN(args.n_envs, input_dims).to(args.device)

  # loss fn
  criterion = torch.nn.MSELoss(reduction='mean')

  trainer = AdaptiveInvariantNNTrainer(model, criterion, args.reg_lambda)
  
  # check if the base classifer match before training
  sampe_n = 100
  x_base_test,y_base_test = env.sample_random_dataset(n = sampe_n)
  x_base_test_sorted = np.sort(x_base_test, axis=0)

  y_base = env.sample_base_classifer(x_base_test_sorted)
  with torch.no_grad(): 
    y_base_predicted = trainer.model.sample_base_classifer(x_base_test_sorted)

  plt.figure()
  plt.plot(x_base_test_sorted[:,0], y_base, label="true base classifer")
  plt.plot(x_base_test_sorted[:,0], y_base_predicted.numpy(), label="estimated base classifer")
  plt.savefig("comparision_before.png")

  # Run Experiment
  print("training")
  # train
  trainer.train(train_dataset, args.batch_size)

  print("test")
  # test
  trainer.test(test_dataset)

  # check if the base classifer match after training
  with torch.no_grad(): 
    y_base_predicted = trainer.model.sample_base_classifer(x_base_test_sorted)

  plt.figure()
  plt.plot(x_base_test_sorted[:,0], y_base, label="true base classifer")
  plt.plot(x_base_test_sorted[:,0], y_base_predicted.numpy(), label="estimated base classifer")
  plt.savefig("comparision_after.png")
